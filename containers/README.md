This directory contains container definitions for services that can be run by Llamaland

# llama-cpp

Provides a generic, hardware-agnostic container for llama.cpp. Only supports CPU and Vulkan backends.

# llama-cpp-cuda

An Nvidia CUDA version of llama.cpp. Supports compute capabilities 7.5 and above.

# llama-cpp-hip